{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# import resources\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "\r\n",
        "# random seed (for reproducibility)\r\n",
        "seed = 1\r\n",
        "# set random seed for numpy\r\n",
        "np.random.seed(seed)\r\n",
        "# set random seed for pytorch\r\n",
        "torch.manual_seed(seed)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x2e7e889a490>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nExVIhReu3IX",
        "outputId": "c077ab39-5192-4b0c-983d-795cbe3c31b9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "from torchvision import datasets\r\n",
        "import torchvision.transforms as transforms\r\n",
        "\r\n",
        "# number of subprocesses to use for data loading\r\n",
        "num_workers = 0\r\n",
        "# how many samples per batch to load\r\n",
        "batch_size = 20\r\n",
        "\r\n",
        "# convert data to Tensors\r\n",
        "transform = transforms.ToTensor()\r\n",
        "\r\n",
        "# choose the training and test datasets\r\n",
        "train_data = datasets.MNIST(root='data', train=True,\r\n",
        "                            download=True, transform=transform)\r\n",
        "\r\n",
        "test_data = datasets.MNIST(root='data', train=False, \r\n",
        "                           download=True, transform=transform)\r\n",
        "\r\n",
        "# prepare data loaders\r\n",
        "train_loader = torch.utils.data.DataLoader(train_data, \r\n",
        "                                           batch_size=batch_size, \r\n",
        "                                           num_workers=num_workers)\r\n",
        "\r\n",
        "test_loader = torch.utils.data.DataLoader(test_data, \r\n",
        "                                          batch_size=batch_size, \r\n",
        "                                          num_workers=num_workers)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\9931168\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ]
        }
      ],
      "metadata": {
        "id": "zAY9BoidvAc9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "    \r\n",
        "# obtain one batch of training images\r\n",
        "dataiter = iter(train_loader)\r\n",
        "images, labels = dataiter.next()\r\n",
        "images = images.numpy()\r\n",
        "\r\n",
        "# plot the images in the batch, along with the corresponding labels\r\n",
        "fig = plt.figure(figsize=(25, 4))\r\n",
        "for idx in np.arange(batch_size):\r\n",
        "    ax = fig.add_subplot(2, batch_size/2, idx+1, xticks=[], yticks=[])\r\n",
        "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\r\n",
        "    # print out the correct label for each image\r\n",
        "    # .item() gets the value contained in a Tensor\r\n",
        "    ax.set_title(str(labels[idx].item()))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\9931168\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:12: MatplotlibDeprecationWarning: Passing non-integers as three-element position specification is deprecated since 3.3 and will be removed two minor releases later.\n",
            "  if sys.path[0] == '':\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1800x288 with 20 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "P3XDuBTOwZA3",
        "outputId": "af9ff40a-5951-4965-ee91-665806e1eee5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F"
      ],
      "outputs": [],
      "metadata": {
        "id": "BXtqPAnVwc8x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "class ConvLayer(nn.Module):\r\n",
        "    \r\n",
        "    def __init__(self, in_channels=1, out_channels=256):\r\n",
        "        '''Constructs the ConvLayer with a specified input and output size.\r\n",
        "           param in_channels: input depth of an image, default value = 1\r\n",
        "           param out_channels: output depth of the convolutional layer, default value = 256\r\n",
        "           '''\r\n",
        "        super(ConvLayer, self).__init__()\r\n",
        "\r\n",
        "        # defining a convolutional layer of the specified size\r\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, \r\n",
        "                              kernel_size=9, stride=1, padding=0)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        '''Defines the feedforward behavior.\r\n",
        "           param x: the input to the layer; an input image\r\n",
        "           return: a relu-activated, convolutional layer\r\n",
        "           '''\r\n",
        "        # applying a ReLu activation to the outputs of the conv layer\r\n",
        "        features = F.relu(self.conv(x)) # will have dimensions (batch_size, 20, 20, 256)\r\n",
        "        return features"
      ],
      "outputs": [],
      "metadata": {
        "id": "f7y0uI3ywf5W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "class PrimaryCaps(nn.Module):\r\n",
        "    \r\n",
        "    def __init__(self, num_capsules=8, in_channels=256, out_channels=32):\r\n",
        "        '''Constructs a list of convolutional layers to be used in \r\n",
        "           creating capsule output vectors.\r\n",
        "           param num_capsules: number of capsules to create\r\n",
        "           param in_channels: input depth of features, default value = 256\r\n",
        "           param out_channels: output depth of the convolutional layers, default value = 32\r\n",
        "           '''\r\n",
        "        super(PrimaryCaps, self).__init__()\r\n",
        "\r\n",
        "        # creating a list of convolutional layers for each capsule I want to create\r\n",
        "        # all capsules have a conv layer with the same parameters\r\n",
        "        self.capsules = nn.ModuleList([\r\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \r\n",
        "                      kernel_size=9, stride=2, padding=0)\r\n",
        "            for _ in range(num_capsules)])\r\n",
        "    \r\n",
        "    def forward(self, x):\r\n",
        "        '''Defines the feedforward behavior.\r\n",
        "           param x: the input; features from a convolutional layer\r\n",
        "           return: a set of normalized, capsule output vectors\r\n",
        "           '''\r\n",
        "        # get batch size of inputs\r\n",
        "        batch_size = x.size(0)\r\n",
        "        # reshape convolutional layer outputs to be (batch_size, vector_dim=1152, 1)\r\n",
        "        u = [capsule(x).view(batch_size, 32 * 6 * 6, 1) for capsule in self.capsules]\r\n",
        "        # stack up output vectors, u, one for each capsule\r\n",
        "        u = torch.cat(u, dim=-1)\r\n",
        "        # squashing the stack of vectors\r\n",
        "        u_squash = self.squash(u)\r\n",
        "        return u_squash\r\n",
        "    \r\n",
        "    def squash(self, input_tensor):\r\n",
        "        '''Squashes an input Tensor so it has a magnitude between 0-1.\r\n",
        "           param input_tensor: a stack of capsule inputs, s_j\r\n",
        "           return: a stack of normalized, capsule output vectors, v_j\r\n",
        "           '''\r\n",
        "        squared_norm = (input_tensor ** 2).sum(dim=-1, keepdim=True)\r\n",
        "        scale = squared_norm / (1 + squared_norm) # normalization coeff\r\n",
        "        output_tensor = scale * input_tensor / torch.sqrt(squared_norm)    \r\n",
        "        return output_tensor"
      ],
      "outputs": [],
      "metadata": {
        "id": "FsvxVCgDwjat"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "import torch\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "def softmax(input_tensor, dim=1):\r\n",
        "    # transpose input\r\n",
        "    transposed_input = input_tensor.transpose(dim, len(input_tensor.size()) - 1)\r\n",
        "    # calculate softmax\r\n",
        "    softmaxed_output = F.softmax(transposed_input.contiguous().view(-1, transposed_input.size(-1)), dim=-1)\r\n",
        "    # un-transpose result\r\n",
        "    return softmaxed_output.view(*transposed_input.size()).transpose(dim, len(input_tensor.size()) - 1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "vYhvYM6szTHX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "#import helpers # to get transpose softmax function\r\n",
        "\r\n",
        "# dynamic routing\r\n",
        "def dynamic_routing(b_ij, u_hat, squash, routing_iterations=3):\r\n",
        "    '''Performs dynamic routing between two capsule layers.\r\n",
        "       param b_ij: initial log probabilities that capsule i should be coupled to capsule j\r\n",
        "       param u_hat: input, weighted capsule vectors, W u\r\n",
        "       param squash: given, normalizing squash function\r\n",
        "       param routing_iterations: number of times to update coupling coefficients\r\n",
        "       return: v_j, output capsule vectors\r\n",
        "       '''    \r\n",
        "    # update b_ij, c_ij for number of routing iterations\r\n",
        "    for iteration in range(routing_iterations):\r\n",
        "        # softmax calculation of coupling coefficients, c_ij\r\n",
        "        c_ij = softmax(b_ij, dim=2)\r\n",
        "         #helpers.\r\n",
        "        \r\n",
        "\r\n",
        "        # calculating total capsule inputs, s_j = sum(c_ij*u_hat)\r\n",
        "        s_j = (c_ij * u_hat).sum(dim=2, keepdim=True)\r\n",
        "\r\n",
        "        # squashing to get a normalized vector output, v_j\r\n",
        "        v_j = squash(s_j)\r\n",
        "\r\n",
        "        # if not on the last iteration, calculate agreement and new b_ij\r\n",
        "        if iteration < routing_iterations - 1:\r\n",
        "            # agreement\r\n",
        "            a_ij = (u_hat * v_j).sum(dim=-1, keepdim=True)\r\n",
        "            \r\n",
        "            # new b_ij\r\n",
        "            b_ij = b_ij + a_ij\r\n",
        "    \r\n",
        "    return v_j # return latest v_j"
      ],
      "outputs": [],
      "metadata": {
        "id": "skUSggXGwm0A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "# it will also be relevant, in this model, to see if I can train on gpu\r\n",
        "TRAIN_ON_GPU = False#torch.cuda.is_available()\r\n",
        "\r\n",
        "if(TRAIN_ON_GPU):\r\n",
        "    print('Training on GPU!')\r\n",
        "else:\r\n",
        "    print('Only CPU available')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only CPU available\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaDbIgfCxER-",
        "outputId": "c421a444-2aa3-4c5a-84f8-a993a958dca9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "class DigitCaps(nn.Module):\r\n",
        "    \r\n",
        "    def __init__(self, num_capsules=10, previous_layer_nodes=32*6*6, \r\n",
        "                 in_channels=8, out_channels=16):\r\n",
        "        '''Constructs an initial weight matrix, W, and sets class variables.\r\n",
        "           param num_capsules: number of capsules to create\r\n",
        "           param previous_layer_nodes: dimension of input capsule vector, default value = 1152\r\n",
        "           param in_channels: number of capsules in previous layer, default value = 8\r\n",
        "           param out_channels: dimensions of output capsule vector, default value = 16\r\n",
        "           '''\r\n",
        "        super(DigitCaps, self).__init__()\r\n",
        "\r\n",
        "        # setting class variables\r\n",
        "        self.num_capsules = num_capsules\r\n",
        "        self.previous_layer_nodes = previous_layer_nodes # vector input (dim=1152)\r\n",
        "        self.in_channels = in_channels # previous layer's number of capsules\r\n",
        "\r\n",
        "        # starting out with a randomly initialized weight matrix, W\r\n",
        "        # these will be the weights connecting the PrimaryCaps and DigitCaps layers\r\n",
        "        self.W = nn.Parameter(torch.randn(num_capsules, previous_layer_nodes, \r\n",
        "                                          in_channels, out_channels))\r\n",
        "\r\n",
        "    def forward(self, u):\r\n",
        "        '''Defines the feedforward behavior.\r\n",
        "           param u: the input; vectors from the previous PrimaryCaps layer\r\n",
        "           return: a set of normalized, capsule output vectors\r\n",
        "           '''\r\n",
        "        \r\n",
        "        # adding batch_size dims and stacking all u vectors\r\n",
        "        u = u[None, :, :, None, :]\r\n",
        "        # 4D weight matrix\r\n",
        "        W = self.W[:, None, :, :, :]\r\n",
        "        \r\n",
        "        # calculating u_hat = W*u\r\n",
        "        u_hat = torch.matmul(u, W)\r\n",
        "\r\n",
        "        # getting the correct size of b_ij\r\n",
        "        # setting them all to 0, initially\r\n",
        "        b_ij = torch.zeros(*u_hat.size())\r\n",
        "        \r\n",
        "        # moving b_ij to GPU, if available\r\n",
        "        if TRAIN_ON_GPU:\r\n",
        "            b_ij = b_ij.cuda()\r\n",
        "\r\n",
        "        # update coupling coefficients and calculate v_j\r\n",
        "        v_j = dynamic_routing(b_ij, u_hat, self.squash, routing_iterations=3)\r\n",
        "\r\n",
        "        return v_j # return final vector outputs\r\n",
        "    \r\n",
        "    \r\n",
        "    def squash(self, input_tensor):\r\n",
        "        '''Squashes an input Tensor so it has a magnitude between 0-1.\r\n",
        "           param input_tensor: a stack of capsule inputs, s_j\r\n",
        "           return: a stack of normalized, capsule output vectors, v_j\r\n",
        "           '''\r\n",
        "        # same squash function as before\r\n",
        "        squared_norm = (input_tensor ** 2).sum(dim=-1, keepdim=True)\r\n",
        "        scale = squared_norm / (1 + squared_norm) # normalization coeff\r\n",
        "        output_tensor = scale * input_tensor / torch.sqrt(squared_norm)    \r\n",
        "        return output_tensor"
      ],
      "outputs": [],
      "metadata": {
        "id": "xGZfUiimxIgX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    \r\n",
        "    def __init__(self, input_vector_length=16, input_capsules=10, hidden_dim=512):\r\n",
        "        '''Constructs an series of linear layers + activations.\r\n",
        "           param input_vector_length: dimension of input capsule vector, default value = 16\r\n",
        "           param input_capsules: number of capsules in previous layer, default value = 10\r\n",
        "           param hidden_dim: dimensions of hidden layers, default value = 512\r\n",
        "           '''\r\n",
        "        super(Decoder, self).__init__()\r\n",
        "        \r\n",
        "        # calculate input_dim\r\n",
        "        input_dim = input_vector_length * input_capsules\r\n",
        "        \r\n",
        "        # define linear layers + activations\r\n",
        "        self.linear_layers = nn.Sequential(\r\n",
        "            nn.Linear(input_dim, hidden_dim), # first hidden layer\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Linear(hidden_dim, hidden_dim*2), # second, twice as deep\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Linear(hidden_dim*2, 28*28), # can be reshaped into 28*28 image\r\n",
        "            nn.Sigmoid() # sigmoid activation to get output pixel values in a range from 0-1\r\n",
        "            )\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        '''Defines the feedforward behavior.\r\n",
        "           param x: the input; vectors from the previous DigitCaps layer\r\n",
        "           return: two things, reconstructed images and the class scores, y\r\n",
        "           '''\r\n",
        "        classes = (x ** 2).sum(dim=-1) ** 0.5\r\n",
        "        classes = F.softmax(classes, dim=-1)\r\n",
        "        \r\n",
        "        # find the capsule with the maximum vector length\r\n",
        "        # here, vector length indicates the probability of a class' existence\r\n",
        "        _, max_length_indices = classes.max(dim=1)\r\n",
        "        \r\n",
        "        # create a sparse class matrix\r\n",
        "        sparse_matrix = torch.eye(10) # 10 is the number of classes\r\n",
        "        if TRAIN_ON_GPU:\r\n",
        "            sparse_matrix = sparse_matrix.cuda()\r\n",
        "        # get the class scores from the \"correct\" capsule\r\n",
        "        y = sparse_matrix.index_select(dim=0, index=max_length_indices.data)\r\n",
        "        \r\n",
        "        # create reconstructed pixels\r\n",
        "        x = x * y[:, :, None]\r\n",
        "        # flatten image into a vector shape (batch_size, vector_dim)\r\n",
        "        flattened_x = x.contiguous().view(x.size(0), -1)\r\n",
        "        # create reconstructed image vectors\r\n",
        "        reconstructions = self.linear_layers(flattened_x)\r\n",
        "        \r\n",
        "        # return reconstructions and the class scores, y\r\n",
        "        return reconstructions, y"
      ],
      "outputs": [],
      "metadata": {
        "id": "Gu8R_RG0xN3y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "class CapsuleNetwork(nn.Module):\r\n",
        "    \r\n",
        "    def __init__(self):\r\n",
        "        '''Constructs a complete Capsule Network.'''\r\n",
        "        super(CapsuleNetwork, self).__init__()\r\n",
        "        self.conv_layer = ConvLayer()\r\n",
        "        self.primary_capsules = PrimaryCaps()\r\n",
        "        self.digit_capsules = DigitCaps()\r\n",
        "        self.decoder = Decoder()\r\n",
        "                \r\n",
        "    def forward(self, images):\r\n",
        "        '''Defines the feedforward behavior.\r\n",
        "           param images: the original MNIST image input data\r\n",
        "           return: output of DigitCaps layer, reconstructed images, class scores\r\n",
        "           '''\r\n",
        "        primary_caps_output = self.primary_capsules(self.conv_layer(images))\r\n",
        "        caps_output = self.digit_capsules(primary_caps_output).squeeze().transpose(0,1)\r\n",
        "        #print(caps_output.type(), caps_output.size())\r\n",
        "        reconstructions, y = self.decoder(caps_output)\r\n",
        "        return caps_output, reconstructions, y"
      ],
      "outputs": [],
      "metadata": {
        "id": "LKr1_RcwxRuE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "# instantiate and print net\r\n",
        "#import torch, gc\r\n",
        "#gc.collect()\r\n",
        "#torch.cuda.empty_cache()\r\n",
        "\r\n",
        "capsule_net = CapsuleNetwork()\r\n",
        "\r\n",
        "print(capsule_net)\r\n",
        "\r\n",
        "\r\n",
        "# move model to GPU, if available \r\n",
        "if TRAIN_ON_GPU:\r\n",
        "    capsule_net = capsule_net.cuda()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CapsuleNetwork(\n",
            "  (conv_layer): ConvLayer(\n",
            "    (conv): Conv2d(1, 256, kernel_size=(9, 9), stride=(1, 1))\n",
            "  )\n",
            "  (primary_capsules): PrimaryCaps(\n",
            "    (capsules): ModuleList(\n",
            "      (0): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "      (1): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "      (2): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "      (3): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "      (4): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "      (5): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "      (6): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "      (7): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "    )\n",
            "  )\n",
            "  (digit_capsules): DigitCaps()\n",
            "  (decoder): Decoder(\n",
            "    (linear_layers): Sequential(\n",
            "      (0): Linear(in_features=160, out_features=512, bias=True)\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): Linear(in_features=1024, out_features=784, bias=True)\n",
            "      (5): Sigmoid()\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHzjE51KxWe3",
        "outputId": "de39ddb8-e603-434a-ce0b-f20854be0d4f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "class CapsuleLoss(nn.Module):\r\n",
        "    \r\n",
        "    def __init__(self):\r\n",
        "        '''Constructs a CapsuleLoss module.'''\r\n",
        "        super(CapsuleLoss, self).__init__()\r\n",
        "        self.reconstruction_loss = nn.MSELoss(reduction='sum') # cumulative loss, equiv to size_average=False\r\n",
        "\r\n",
        "    def forward(self, x, labels, images, reconstructions):\r\n",
        "        '''Defines how the loss compares inputs.\r\n",
        "           param x: digit capsule outputs\r\n",
        "           param labels: \r\n",
        "           param images: the original MNIST image input data\r\n",
        "           param reconstructions: reconstructed MNIST image data\r\n",
        "           return: weighted margin and reconstruction loss, averaged over a batch\r\n",
        "           '''\r\n",
        "        batch_size = x.size(0)\r\n",
        "\r\n",
        "        ##  calculate the margin loss   ##\r\n",
        "        \r\n",
        "        # get magnitude of digit capsule vectors, v_c\r\n",
        "        v_c = torch.sqrt((x**2).sum(dim=2, keepdim=True))\r\n",
        "\r\n",
        "        # calculate \"correct\" and incorrect loss\r\n",
        "        left = F.relu(0.9 - v_c).view(batch_size, -1)\r\n",
        "        right = F.relu(v_c - 0.1).view(batch_size, -1)\r\n",
        "        \r\n",
        "        # sum the losses, with a lambda = 0.5\r\n",
        "        margin_loss = labels * left + 0.5 * (1. - labels) * right\r\n",
        "        margin_loss = margin_loss.sum()\r\n",
        "\r\n",
        "        ##  calculate the reconstruction loss   ##\r\n",
        "        images = images.view(reconstructions.size()[0], -1)\r\n",
        "        reconstruction_loss = self.reconstruction_loss(reconstructions, images)\r\n",
        "\r\n",
        "        # return a weighted, summed loss, averaged over a batch size\r\n",
        "        return (margin_loss + 0.0005 * reconstruction_loss) / images.size(0)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Lf7jUp2exZQP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "import torch.optim as optim\r\n",
        "\r\n",
        "# custom loss\r\n",
        "criterion = CapsuleLoss()\r\n",
        "\r\n",
        "# Adam optimizer with default params\r\n",
        "optimizer = optim.Adam(capsule_net.parameters())"
      ],
      "outputs": [],
      "metadata": {
        "id": "GYecVY8Jxdtt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "def train(capsule_net, criterion, optimizer, \r\n",
        "          n_epochs, print_every=300):\r\n",
        "    '''Trains a capsule network and prints out training batch loss statistics.\r\n",
        "       Saves model parameters if *validation* loss has decreased.\r\n",
        "       param capsule_net: trained capsule network\r\n",
        "       param criterion: capsule loss function\r\n",
        "       param optimizer: optimizer for updating network weights\r\n",
        "       param n_epochs: number of epochs to train for\r\n",
        "       param print_every: batches to print and save training loss, default = 100\r\n",
        "       return: list of recorded training losses\r\n",
        "       '''\r\n",
        "\r\n",
        "    # track training loss over time\r\n",
        "    losses = []\r\n",
        "\r\n",
        "    # one epoch = one pass over all training data \r\n",
        "    for epoch in range(1, n_epochs+1):\r\n",
        "\r\n",
        "        # initialize training loss\r\n",
        "        train_loss = 0.0\r\n",
        "        \r\n",
        "        capsule_net.train() # set to train mode\r\n",
        "    \r\n",
        "        # get batches of training image data and targets\r\n",
        "        for batch_i, (images, target) in enumerate(train_loader):\r\n",
        "\r\n",
        "            # reshape and get target class\r\n",
        "            target = torch.eye(10).index_select(dim=0, index=target)\r\n",
        "\r\n",
        "            if TRAIN_ON_GPU:\r\n",
        "                images, target = images.cuda(), target.cuda()\r\n",
        "\r\n",
        "            # zero out gradients\r\n",
        "            optimizer.zero_grad()\r\n",
        "            # get model outputs\r\n",
        "            caps_output, reconstructions, y = capsule_net(images)\r\n",
        "            # calculate loss\r\n",
        "            loss = criterion(caps_output, target, images, reconstructions)\r\n",
        "            # perform backpropagation and optimization\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "\r\n",
        "            train_loss += loss.item() # accumulated training loss\r\n",
        "            \r\n",
        "            # print and record training stats\r\n",
        "            if batch_i != 0 and batch_i % print_every == 0:\r\n",
        "                avg_train_loss = train_loss/print_every\r\n",
        "                losses.append(avg_train_loss)\r\n",
        "                print('Epoch: {} \\tTraining Loss: {:.8f}'.format(epoch, avg_train_loss))\r\n",
        "                train_loss = 0 # reset accumulated training loss\r\n",
        "        \r\n",
        "    return losses"
      ],
      "outputs": [],
      "metadata": {
        "id": "V0rVHcOnxjE1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# training for 3 epochs\r\n",
        "n_epochs = 3\r\n",
        "losses = train(capsule_net, criterion, optimizer, n_epochs=n_epochs)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQs6bM2gxoCT",
        "outputId": "5b77a345-825f-4929-afac-2807074b2e59"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "def test(capsule_net, test_loader):\r\n",
        "    '''Prints out test statistics for a given capsule net.\r\n",
        "       param capsule_net: trained capsule network\r\n",
        "       param test_loader: test dataloader\r\n",
        "       return: returns last batch of test image data and corresponding reconstructions\r\n",
        "       '''\r\n",
        "    class_correct = list(0. for i in range(10))\r\n",
        "    class_total = list(0. for i in range(10))\r\n",
        "    \r\n",
        "    test_loss = 0 # loss tracking\r\n",
        "\r\n",
        "    capsule_net.eval() # eval mode\r\n",
        "\r\n",
        "    for batch_i, (images, target) in enumerate(test_loader):\r\n",
        "        target = torch.eye(10).index_select(dim=0, index=target)\r\n",
        "\r\n",
        "        batch_size = images.size(0)\r\n",
        "\r\n",
        "        if TRAIN_ON_GPU:\r\n",
        "            images, target = images.cuda(), target.cuda()\r\n",
        "\r\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\r\n",
        "        caps_output, reconstructions, y = capsule_net(images)\r\n",
        "        # calculate the loss\r\n",
        "        loss = criterion(caps_output, target, images, reconstructions)\r\n",
        "        # update average test loss \r\n",
        "        test_loss += loss.item()\r\n",
        "        # convert output probabilities to predicted class\r\n",
        "        _, pred = torch.max(y.data.cpu(), 1)\r\n",
        "        _, target_shape = torch.max(target.data.cpu(), 1)\r\n",
        "\r\n",
        "        # compare predictions to true label\r\n",
        "        correct = np.squeeze(pred.eq(target_shape.data.view_as(pred)))\r\n",
        "        # calculate test accuracy for each object class\r\n",
        "        for i in range(batch_size):\r\n",
        "            label = target_shape.data[i]\r\n",
        "            class_correct[label] += correct[i].item()\r\n",
        "            class_total[label] += 1\r\n",
        "\r\n",
        "    # avg test loss\r\n",
        "    avg_test_loss = test_loss/len(test_loader)\r\n",
        "    print('Test Loss: {:.8f}\\n'.format(avg_test_loss))\r\n",
        "\r\n",
        "    for i in range(10):\r\n",
        "        if class_total[i] > 0:\r\n",
        "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\r\n",
        "                str(i), 100 * class_correct[i] / class_total[i],\r\n",
        "                np.sum(class_correct[i]), np.sum(class_total[i])))\r\n",
        "        else:\r\n",
        "            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\r\n",
        "\r\n",
        "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\r\n",
        "        100. * np.sum(class_correct) / np.sum(class_total),\r\n",
        "        np.sum(class_correct), np.sum(class_total)))\r\n",
        "    \r\n",
        "    # return last batch of capsule vectors, images, reconstructions\r\n",
        "    return caps_output, images, reconstructions"
      ],
      "outputs": [],
      "metadata": {
        "id": "oR6Gu9yQyE_U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "source": [
        "#load the model\r\n",
        "\r\n",
        "\r\n",
        "from torchsummary import summary\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "#device = torch.device('cpu')\r\n",
        "\r\n",
        "\r\n",
        "capsule_net.load_state_dict(torch.load('./model-parameters.pt'))\r\n",
        "\r\n",
        "capsule_net = CapsuleNetwork().to(device)\r\n",
        "\r\n",
        "\r\n",
        "capsule_net.eval()\r\n",
        "pre_trained_dict = capsule_net.state_dict()\r\n",
        "\r\n",
        "mystr =  'primary_capsules.capsules.' + str(7) + '.bias'\r\n",
        "print(mystr)\r\n",
        "primary_caps = pre_trained_dict[mystr][0].item()\r\n",
        "\r\n",
        "print(type(primary_caps))\r\n",
        "\r\n",
        "weight_tensor = pre_trained_dict['digit_capsules.W']\r\n",
        "\r\n",
        "print(weight_tensor.size())\r\n",
        "\r\n",
        "#print(weight_tensor.size())\r\n",
        "#print(weight_tensor)\r\n",
        "\r\n",
        "#print('none zero weights count: ', torch.count_nonzero(weight_tensor))\r\n",
        "#print('number of all tensor cells: ', torch.numel(weight_tensor))\r\n",
        "\r\n",
        "#pre_trained_dict['primary_capsules.capsules.1.weight'] = torch.zeros(32, 256, 9, 9, dtype=torch.float)\r\n",
        "\r\n",
        "#conv_weights = a\r\n",
        "#print(a.size())\r\n",
        "#print(a)\r\n",
        "#print(pre_trained_dict['digit_capsules.W'])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#PRINTING NETWORK PARAMETERS\r\n",
        "#for name, param in capsule_net.named_parameters():\r\n",
        "#    if param.requires_grad:\r\n",
        "#        print(name, param.type())\r\n",
        "\r\n",
        "#summary(capsule_net, input_size=(1, 28, 28))\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "primary_capsules.capsules.7.bias\n",
            "<class 'float'>\n",
            "torch.Size([10, 1152, 8, 16])\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "source": [
        "%%time\r\n",
        "# call test function and get reconstructed images\r\n",
        "capsule_net.load_state_dict(torch.load('./model-parameters.pt'))\r\n",
        "capsule_net.eval()\r\n",
        "caps_output, images, reconstructions = test(capsule_net, test_loader)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.03458104\n",
            "\n",
            "Test Accuracy of     0: 99% (975/980)\n",
            "Test Accuracy of     1: 99% (1130/1135)\n",
            "Test Accuracy of     2: 99% (1024/1032)\n",
            "Test Accuracy of     3: 98% (998/1010)\n",
            "Test Accuracy of     4: 99% (976/982)\n",
            "Test Accuracy of     5: 99% (888/892)\n",
            "Test Accuracy of     6: 99% (951/958)\n",
            "Test Accuracy of     7: 99% (1019/1028)\n",
            "Test Accuracy of     8: 98% (964/974)\n",
            "Test Accuracy of     9: 96% (977/1009)\n",
            "\n",
            "Test Accuracy (Overall): 99% (9902/10000)\n",
            "Wall time: 47.9 s\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clQUkYe81d3Q",
        "outputId": "2c167fd7-1ca2-4096-9349-ae86ff972989"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import struct\r\n",
        "def float_to_bin(num):\r\n",
        "    return format(struct.unpack('!I', struct.pack('!f', num))[0], '032b')\r\n",
        "\r\n",
        "def bin_to_float(binary):\r\n",
        "    return struct.unpack('!f',struct.pack('!I', int(binary, 2)))[0]"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "%%time\r\n",
        "from utils import float2bit\r\n",
        "from utils import bit2float\r\n",
        "import math\r\n",
        "\r\n",
        "#capsule_net.load_state_dict(torch.load('./model-parameters.pt'))\r\n",
        "\r\n",
        "#capsule_net.eval()\r\n",
        "\r\n",
        "#summary(capsule_net, input_size=(1, 28, 28))\r\n",
        "\r\n",
        "#state_dict = capsule_net.state_dict()\r\n",
        "\r\n",
        "\r\n",
        "#print(state_dict['digit_capsules.W'][0, 0, 0, 0], '\\n')\r\n",
        "\r\n",
        "\r\n",
        "#binary_tensor = float2bit(state_dict['digit_capsules.W'], num_e_bits=8, num_m_bits=23, bias=127.)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#print(torch.reshape(binary_tensor, (-1,)).size(), '\\n')\r\n",
        "\r\n",
        "#print(torch.reshape(binary_tensor, (-1,))[0].item(), '\\n')\r\n",
        "\r\n",
        "#torch.reshape(binary_tensor, (-1,))[0] = float(0)\r\n",
        "\r\n",
        "#print(torch.reshape(binary_tensor, (-1,))[0].item(), '\\n')\r\n",
        "\r\n",
        "#print(type(torch.reshape(binary_tensor, (-1,))[47185919].item()), '\\n')\r\n",
        "\r\n",
        "\r\n",
        "# validity test  ###############################################\r\n",
        "\r\n",
        "#for i in range (1, 9):\r\n",
        "    #torch.reshape(state_dict['digit_capsules.W'] , (-1,))[random_tensor_index].item()\r\n",
        " #   binary_tensor[0, 0, 0, 0][i] = float(1)\r\n",
        "\r\n",
        "#for i in range (9, 32):\r\n",
        "#    binary_tensor[0, 0, 0, 0][i] = float(0)\r\n",
        "\r\n",
        "\r\n",
        "#print((binary_tensor[0, 0, 0, 0]), '\\n')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#float_tensor = bit2float(binary_tensor, num_e_bits=8, num_m_bits=23, bias=127.)\r\n",
        "\r\n",
        "#print(math.isfinite(float_tensor[0, 0, 0, 0]))\r\n",
        "\r\n",
        "\r\n",
        "#print(float_tensor[0, 0, 0, 0])\r\n",
        "\r\n",
        "##################################################################\r\n",
        "\r\n",
        "\r\n",
        "## Fault injection ###############################################\r\n",
        "\r\n",
        "import random\r\n",
        "\r\n",
        "number_of_bit_flips = 1000000\r\n",
        "\r\n",
        "exponential_fault_growth = 113000000\r\n",
        "\r\n",
        "while exponential_fault_growth < 169869311:\r\n",
        "\r\n",
        "    capsule_net.load_state_dict(torch.load('./model-parameters.pt'))\r\n",
        "\r\n",
        "    capsule_net.eval()\r\n",
        "\r\n",
        "    state_dict = capsule_net.state_dict()\r\n",
        "\r\n",
        "    #binary_primary_caps_0 = float2bit(state_dict['primary_capsules.capsules.0.weight'], num_e_bits=8, num_m_bits=23, bias=127.)\r\n",
        "\r\n",
        "    #binary_primary_caps_1 = float2bit(state_dict['primary_capsules.capsules.1.weight'], num_e_bits=8, num_m_bits=23, bias=127.)\r\n",
        "\r\n",
        "    #binary_primary_caps_2 = float2bit(state_dict['primary_capsules.capsules.2.weight'], num_e_bits=8, num_m_bits=23, bias=127.)\r\n",
        "\r\n",
        "    #binary_primary_caps_3 = float2bit(state_dict['primary_capsules.capsules.3.weight'], num_e_bits=8, num_m_bits=23, bias=127.)\r\n",
        "\r\n",
        "    #binary_primary_caps_4 = float2bit(state_dict['primary_capsules.capsules.4.weight'], num_e_bits=8, num_m_bits=23, bias=127.)\r\n",
        "\r\n",
        "    #binary_primary_caps_5 = float2bit(state_dict['primary_capsules.capsules.5.weight'], num_e_bits=8, num_m_bits=23, bias=127.)\r\n",
        "\r\n",
        "    #binary_primary_caps_6 = float2bit(state_dict['primary_capsules.capsules.6.weight'], num_e_bits=8, num_m_bits=23, bias=127.)\r\n",
        "\r\n",
        "    #binary_primary_caps_7 = float2bit(state_dict['primary_capsules.capsules.7.weight'], num_e_bits=8, num_m_bits=23, bias=127.)\r\n",
        "\r\n",
        "    #binary_tensor = float2bit(state_dict['digit_capsules.W'], num_e_bits=8, num_m_bits=23, bias=127.)\r\n",
        "\r\n",
        "    temp = torch.cat((state_dict['primary_capsules.capsules.0.weight'],\r\n",
        "                      state_dict['primary_capsules.capsules.1.weight'],\r\n",
        "                      state_dict['primary_capsules.capsules.2.weight'],\r\n",
        "                      state_dict['primary_capsules.capsules.3.weight'],\r\n",
        "                      state_dict['primary_capsules.capsules.4.weight'],\r\n",
        "                      state_dict['primary_capsules.capsules.5.weight'],\r\n",
        "                      state_dict['primary_capsules.capsules.6.weight'], \r\n",
        "                      state_dict['primary_capsules.capsules.7.weight']), dim=0)\r\n",
        "\r\n",
        "    primary_caps_tensor = float2bit(temp, num_e_bits=8, num_m_bits=23, bias=127.)\r\n",
        "\r\n",
        "    \r\n",
        "    for i in range (exponential_fault_growth):\r\n",
        "\r\n",
        "        random_bit_number = random.randint(0, 169869311)  \r\n",
        "\r\n",
        "        if( torch.reshape(primary_caps_tensor, (-1,))[random_bit_number] == float(0) ):\r\n",
        "\r\n",
        "            torch.reshape(primary_caps_tensor, (-1,))[random_bit_number] = float(1)\r\n",
        "\r\n",
        "        if( torch.reshape(primary_caps_tensor, (-1,))[random_bit_number] == float(1) ):\r\n",
        "\r\n",
        "            torch.reshape(primary_caps_tensor, (-1,))[random_bit_number] = float(0)\r\n",
        "\r\n",
        "\r\n",
        "    print(\"Number of Faults Injected: \", exponential_fault_growth, '\\n')\r\n",
        "\r\n",
        "    exponential_fault_growth = exponential_fault_growth + 1000000\r\n",
        "\r\n",
        "    float_tensor = bit2float(primary_caps_tensor, num_e_bits=8, num_m_bits=23, bias=127.)\r\n",
        "\r\n",
        "    final_tensor = torch.split(float_tensor, 32)\r\n",
        "\r\n",
        "    state_dict['primary_capsules.capsules.0.weight'] = final_tensor[0]\r\n",
        "    state_dict['primary_capsules.capsules.1.weight'] = final_tensor[1]\r\n",
        "    state_dict['primary_capsules.capsules.2.weight'] = final_tensor[2]\r\n",
        "    state_dict['primary_capsules.capsules.3.weight'] = final_tensor[3]\r\n",
        "    state_dict['primary_capsules.capsules.4.weight'] = final_tensor[4]\r\n",
        "    state_dict['primary_capsules.capsules.5.weight'] = final_tensor[5]\r\n",
        "    state_dict['primary_capsules.capsules.6.weight'] = final_tensor[6]\r\n",
        "    state_dict['primary_capsules.capsules.7.weight'] = final_tensor[7]\r\n",
        "\r\n",
        "    #for i in range (6631551):\r\n",
        "    #    if (math.isfinite(torch.reshape(float_tensor, (-1,))[i]) == False):\r\n",
        "    #     print(\"EROOOOOOOOOOOOOOOOOR\")\r\n",
        "\r\n",
        "    #state_dict['digit_capsules.W'] = float_tensor\r\n",
        "    capsule_net.load_state_dict(state_dict)\r\n",
        "    capsule_net.eval()\r\n",
        "    caps_output, images, reconstructions = test(capsule_net, test_loader)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    print(\"*************************************************************************\", '\\n')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Faults Injected:  113000000 \n",
            "\n",
            "Test Loss: 0.91409494\n",
            "\n",
            "Test Accuracy of     0: 96% (944/980)\n",
            "Test Accuracy of     1: 22% (256/1135)\n",
            "Test Accuracy of     2: 88% (912/1032)\n",
            "Test Accuracy of     3: 41% (417/1010)\n",
            "Test Accuracy of     4: 99% (980/982)\n",
            "Test Accuracy of     5: 85% (760/892)\n",
            "Test Accuracy of     6: 97% (938/958)\n",
            "Test Accuracy of     7: 87% (903/1028)\n",
            "Test Accuracy of     8: 98% (957/974)\n",
            "Test Accuracy of     9: 91% (920/1009)\n",
            "\n",
            "Test Accuracy (Overall): 79% (7987/10000)\n",
            "************************************************************************* \n",
            "\n",
            "Number of Faults Injected:  114000000 \n",
            "\n",
            "Test Loss: 0.91523761\n",
            "\n",
            "Test Accuracy of     0: 98% (966/980)\n",
            "Test Accuracy of     1: 32% (368/1135)\n",
            "Test Accuracy of     2: 79% (820/1032)\n",
            "Test Accuracy of     3:  8% (84/1010)\n",
            "Test Accuracy of     4: 99% (978/982)\n",
            "Test Accuracy of     5: 93% (830/892)\n",
            "Test Accuracy of     6: 97% (934/958)\n",
            "Test Accuracy of     7: 94% (972/1028)\n",
            "Test Accuracy of     8: 98% (960/974)\n",
            "Test Accuracy of     9: 71% (717/1009)\n",
            "\n",
            "Test Accuracy (Overall): 76% (7629/10000)\n",
            "************************************************************************* \n",
            "\n",
            "Number of Faults Injected:  115000000 \n",
            "\n",
            "Test Loss: 0.91552201\n",
            "\n",
            "Test Accuracy of     0: 97% (954/980)\n",
            "Test Accuracy of     1: 54% (613/1135)\n",
            "Test Accuracy of     2: 68% (704/1032)\n",
            "Test Accuracy of     3: 24% (244/1010)\n",
            "Test Accuracy of     4: 99% (981/982)\n",
            "Test Accuracy of     5: 91% (818/892)\n",
            "Test Accuracy of     6: 97% (937/958)\n",
            "Test Accuracy of     7: 90% (933/1028)\n",
            "Test Accuracy of     8: 98% (956/974)\n",
            "Test Accuracy of     9: 58% (595/1009)\n",
            "\n",
            "Test Accuracy (Overall): 77% (7735/10000)\n",
            "************************************************************************* \n",
            "\n",
            "Number of Faults Injected:  116000000 \n",
            "\n",
            "Test Loss: 0.91633586\n",
            "\n",
            "Test Accuracy of     0: 96% (945/980)\n",
            "Test Accuracy of     1: 13% (149/1135)\n",
            "Test Accuracy of     2: 75% (778/1032)\n",
            "Test Accuracy of     3: 15% (161/1010)\n",
            "Test Accuracy of     4: 99% (979/982)\n",
            "Test Accuracy of     5: 85% (759/892)\n",
            "Test Accuracy of     6: 96% (928/958)\n",
            "Test Accuracy of     7: 90% (928/1028)\n",
            "Test Accuracy of     8: 98% (958/974)\n",
            "Test Accuracy of     9: 71% (722/1009)\n",
            "\n",
            "Test Accuracy (Overall): 73% (7307/10000)\n",
            "************************************************************************* \n",
            "\n",
            "Number of Faults Injected:  117000000 \n",
            "\n",
            "Test Loss: 0.91695395\n",
            "\n",
            "Test Accuracy of     0: 95% (938/980)\n",
            "Test Accuracy of     1: 20% (235/1135)\n",
            "Test Accuracy of     2: 69% (717/1032)\n",
            "Test Accuracy of     3: 16% (162/1010)\n",
            "Test Accuracy of     4: 100% (982/982)\n",
            "Test Accuracy of     5: 86% (769/892)\n",
            "Test Accuracy of     6: 96% (926/958)\n",
            "Test Accuracy of     7: 95% (980/1028)\n",
            "Test Accuracy of     8: 98% (956/974)\n",
            "Test Accuracy of     9: 77% (781/1009)\n",
            "\n",
            "Test Accuracy (Overall): 74% (7446/10000)\n",
            "************************************************************************* \n",
            "\n",
            "Number of Faults Injected:  118000000 \n",
            "\n",
            "Test Loss: 0.91802831\n",
            "\n",
            "Test Accuracy of     0: 97% (958/980)\n",
            "Test Accuracy of     1: 21% (241/1135)\n",
            "Test Accuracy of     2: 61% (633/1032)\n",
            "Test Accuracy of     3:  0% ( 8/1010)\n",
            "Test Accuracy of     4: 100% (982/982)\n",
            "Test Accuracy of     5: 73% (658/892)\n",
            "Test Accuracy of     6: 95% (918/958)\n",
            "Test Accuracy of     7: 85% (884/1028)\n",
            "Test Accuracy of     8: 97% (950/974)\n",
            "Test Accuracy of     9: 78% (790/1009)\n",
            "\n",
            "Test Accuracy (Overall): 70% (7022/10000)\n",
            "************************************************************************* \n",
            "\n",
            "Number of Faults Injected:  119000000 \n",
            "\n",
            "Test Loss: 0.91844263\n",
            "\n",
            "Test Accuracy of     0: 96% (942/980)\n",
            "Test Accuracy of     1:  2% (30/1135)\n",
            "Test Accuracy of     2: 75% (781/1032)\n",
            "Test Accuracy of     3:  2% (25/1010)\n",
            "Test Accuracy of     4: 99% (977/982)\n",
            "Test Accuracy of     5: 85% (764/892)\n",
            "Test Accuracy of     6: 94% (905/958)\n",
            "Test Accuracy of     7: 81% (840/1028)\n",
            "Test Accuracy of     8: 96% (940/974)\n",
            "Test Accuracy of     9: 63% (637/1009)\n",
            "\n",
            "Test Accuracy (Overall): 68% (6841/10000)\n",
            "************************************************************************* \n",
            "\n",
            "Number of Faults Injected:  120000000 \n",
            "\n",
            "Test Loss: 0.91878661\n",
            "\n",
            "Test Accuracy of     0: 97% (952/980)\n",
            "Test Accuracy of     1: 26% (299/1135)\n",
            "Test Accuracy of     2: 61% (638/1032)\n",
            "Test Accuracy of     3: 20% (202/1010)\n",
            "Test Accuracy of     4: 99% (978/982)\n",
            "Test Accuracy of     5: 84% (751/892)\n",
            "Test Accuracy of     6: 95% (919/958)\n",
            "Test Accuracy of     7: 79% (817/1028)\n",
            "Test Accuracy of     8: 98% (963/974)\n",
            "Test Accuracy of     9: 69% (704/1009)\n",
            "\n",
            "Test Accuracy (Overall): 72% (7223/10000)\n",
            "************************************************************************* \n",
            "\n",
            "Number of Faults Injected:  121000000 \n",
            "\n",
            "Test Loss: 0.91868226\n",
            "\n",
            "Test Accuracy of     0: 93% (919/980)\n",
            "Test Accuracy of     1: 30% (344/1135)\n",
            "Test Accuracy of     2: 63% (660/1032)\n",
            "Test Accuracy of     3:  9% (95/1010)\n",
            "Test Accuracy of     4: 99% (979/982)\n",
            "Test Accuracy of     5: 86% (769/892)\n",
            "Test Accuracy of     6: 96% (921/958)\n",
            "Test Accuracy of     7: 94% (970/1028)\n",
            "Test Accuracy of     8: 98% (963/974)\n",
            "Test Accuracy of     9: 78% (789/1009)\n",
            "\n",
            "Test Accuracy (Overall): 74% (7409/10000)\n",
            "************************************************************************* \n",
            "\n",
            "Number of Faults Injected:  122000000 \n",
            "\n",
            "Test Loss: 0.91918445\n",
            "\n",
            "Test Accuracy of     0: 91% (894/980)\n",
            "Test Accuracy of     1: 24% (282/1135)\n",
            "Test Accuracy of     2: 78% (813/1032)\n",
            "Test Accuracy of     3:  2% (22/1010)\n",
            "Test Accuracy of     4: 99% (976/982)\n",
            "Test Accuracy of     5: 78% (699/892)\n",
            "Test Accuracy of     6: 92% (885/958)\n",
            "Test Accuracy of     7: 90% (927/1028)\n",
            "Test Accuracy of     8: 97% (954/974)\n",
            "Test Accuracy of     9: 77% (783/1009)\n",
            "\n",
            "Test Accuracy (Overall): 72% (7235/10000)\n",
            "************************************************************************* \n",
            "\n",
            "Number of Faults Injected:  123000000 \n",
            "\n",
            "Test Loss: 0.91979466\n",
            "\n",
            "Test Accuracy of     0: 95% (939/980)\n",
            "Test Accuracy of     1: 27% (311/1135)\n",
            "Test Accuracy of     2: 68% (705/1032)\n",
            "Test Accuracy of     3:  7% (78/1010)\n",
            "Test Accuracy of     4: 99% (981/982)\n",
            "Test Accuracy of     5: 89% (795/892)\n",
            "Test Accuracy of     6: 95% (918/958)\n",
            "Test Accuracy of     7: 85% (878/1028)\n",
            "Test Accuracy of     8: 98% (960/974)\n",
            "Test Accuracy of     9: 77% (778/1009)\n",
            "\n",
            "Test Accuracy (Overall): 73% (7343/10000)\n",
            "************************************************************************* \n",
            "\n",
            "Number of Faults Injected:  124000000 \n",
            "\n",
            "Test Loss: 0.92076204\n",
            "\n",
            "Test Accuracy of     0: 93% (915/980)\n",
            "Test Accuracy of     1:  3% (39/1135)\n",
            "Test Accuracy of     2: 47% (491/1032)\n",
            "Test Accuracy of     3:  1% (16/1010)\n",
            "Test Accuracy of     4: 99% (976/982)\n",
            "Test Accuracy of     5: 76% (684/892)\n",
            "Test Accuracy of     6: 86% (827/958)\n",
            "Test Accuracy of     7: 83% (858/1028)\n",
            "Test Accuracy of     8: 97% (950/974)\n",
            "Test Accuracy of     9: 77% (781/1009)\n",
            "\n",
            "Test Accuracy (Overall): 65% (6537/10000)\n",
            "************************************************************************* \n",
            "\n",
            "Number of Faults Injected:  125000000 \n",
            "\n",
            "Test Loss: 0.92050512\n",
            "\n",
            "Test Accuracy of     0: 90% (888/980)\n",
            "Test Accuracy of     1:  9% (103/1135)\n",
            "Test Accuracy of     2: 66% (690/1032)\n",
            "Test Accuracy of     3:  3% (35/1010)\n",
            "Test Accuracy of     4: 98% (972/982)\n",
            "Test Accuracy of     5: 81% (725/892)\n",
            "Test Accuracy of     6: 94% (906/958)\n",
            "Test Accuracy of     7: 81% (838/1028)\n",
            "Test Accuracy of     8: 98% (958/974)\n",
            "Test Accuracy of     9: 78% (792/1009)\n",
            "\n",
            "Test Accuracy (Overall): 69% (6907/10000)\n",
            "************************************************************************* \n",
            "\n",
            "Number of Faults Injected:  126000000 \n",
            "\n",
            "Test Loss: 0.92111026\n",
            "\n",
            "Test Accuracy of     0: 94% (926/980)\n",
            "Test Accuracy of     1: 45% (522/1135)\n",
            "Test Accuracy of     2: 67% (698/1032)\n",
            "Test Accuracy of     3:  1% (15/1010)\n",
            "Test Accuracy of     4: 99% (974/982)\n",
            "Test Accuracy of     5: 75% (676/892)\n",
            "Test Accuracy of     6: 93% (892/958)\n",
            "Test Accuracy of     7: 84% (870/1028)\n",
            "Test Accuracy of     8: 98% (957/974)\n",
            "Test Accuracy of     9: 66% (666/1009)\n",
            "\n",
            "Test Accuracy (Overall): 71% (7196/10000)\n",
            "************************************************************************* \n",
            "\n",
            "Number of Faults Injected:  127000000 \n",
            "\n",
            "Test Loss: 0.92118699\n",
            "\n",
            "Test Accuracy of     0: 85% (839/980)\n",
            "Test Accuracy of     1: 52% (598/1135)\n",
            "Test Accuracy of     2: 54% (563/1032)\n",
            "Test Accuracy of     3:  0% ( 1/1010)\n",
            "Test Accuracy of     4: 97% (956/982)\n",
            "Test Accuracy of     5: 79% (707/892)\n",
            "Test Accuracy of     6: 90% (865/958)\n",
            "Test Accuracy of     7: 86% (892/1028)\n",
            "Test Accuracy of     8: 97% (952/974)\n",
            "Test Accuracy of     9: 73% (740/1009)\n",
            "\n",
            "Test Accuracy (Overall): 71% (7113/10000)\n",
            "************************************************************************* \n",
            "\n",
            "Number of Faults Injected:  128000000 \n",
            "\n",
            "Test Loss: 0.92195380\n",
            "\n",
            "Test Accuracy of     0: 88% (865/980)\n",
            "Test Accuracy of     1: 24% (279/1135)\n",
            "Test Accuracy of     2: 42% (437/1032)\n",
            "Test Accuracy of     3:  2% (30/1010)\n",
            "Test Accuracy of     4: 99% (979/982)\n",
            "Test Accuracy of     5: 68% (607/892)\n",
            "Test Accuracy of     6: 94% (901/958)\n",
            "Test Accuracy of     7: 76% (785/1028)\n",
            "Test Accuracy of     8: 98% (957/974)\n",
            "Test Accuracy of     9: 53% (536/1009)\n",
            "\n",
            "Test Accuracy (Overall): 63% (6376/10000)\n",
            "************************************************************************* \n",
            "\n",
            "Number of Faults Injected:  129000000 \n",
            "\n",
            "Test Loss: 0.92211923\n",
            "\n",
            "Test Accuracy of     0: 93% (919/980)\n",
            "Test Accuracy of     1:  5% (62/1135)\n",
            "Test Accuracy of     2: 46% (479/1032)\n",
            "Test Accuracy of     3:  1% (11/1010)\n",
            "Test Accuracy of     4: 99% (974/982)\n",
            "Test Accuracy of     5: 40% (357/892)\n",
            "Test Accuracy of     6: 91% (875/958)\n",
            "Test Accuracy of     7: 78% (805/1028)\n",
            "Test Accuracy of     8: 98% (961/974)\n",
            "Test Accuracy of     9: 51% (517/1009)\n",
            "\n",
            "Test Accuracy (Overall): 59% (5960/10000)\n",
            "************************************************************************* \n",
            "\n",
            "Number of Faults Injected:  130000000 \n",
            "\n",
            "Test Loss: 0.92222113\n",
            "\n",
            "Test Accuracy of     0: 76% (754/980)\n",
            "Test Accuracy of     1:  4% (55/1135)\n",
            "Test Accuracy of     2: 20% (209/1032)\n",
            "Test Accuracy of     3:  1% (19/1010)\n",
            "Test Accuracy of     4: 98% (968/982)\n",
            "Test Accuracy of     5: 55% (492/892)\n",
            "Test Accuracy of     6: 87% (837/958)\n",
            "Test Accuracy of     7: 69% (718/1028)\n",
            "Test Accuracy of     8: 99% (968/974)\n",
            "Test Accuracy of     9: 75% (765/1009)\n",
            "\n",
            "Test Accuracy (Overall): 57% (5785/10000)\n",
            "************************************************************************* \n",
            "\n",
            "Number of Faults Injected:  131000000 \n",
            "\n",
            "Test Loss: 0.92236135\n",
            "\n",
            "Test Accuracy of     0: 76% (753/980)\n",
            "Test Accuracy of     1: 14% (169/1135)\n",
            "Test Accuracy of     2: 27% (285/1032)\n",
            "Test Accuracy of     3: 10% (104/1010)\n",
            "Test Accuracy of     4: 98% (967/982)\n",
            "Test Accuracy of     5: 82% (737/892)\n",
            "Test Accuracy of     6: 93% (896/958)\n",
            "Test Accuracy of     7: 81% (836/1028)\n",
            "Test Accuracy of     8: 99% (969/974)\n",
            "Test Accuracy of     9: 72% (729/1009)\n",
            "\n",
            "Test Accuracy (Overall): 64% (6445/10000)\n",
            "************************************************************************* \n",
            "\n",
            "Number of Faults Injected:  132000000 \n",
            "\n",
            "Test Loss: 0.92308117\n",
            "\n",
            "Test Accuracy of     0: 85% (838/980)\n",
            "Test Accuracy of     1:  2% (23/1135)\n",
            "Test Accuracy of     2: 20% (210/1032)\n",
            "Test Accuracy of     3:  0% ( 8/1010)\n",
            "Test Accuracy of     4: 97% (960/982)\n",
            "Test Accuracy of     5: 22% (201/892)\n",
            "Test Accuracy of     6: 87% (843/958)\n",
            "Test Accuracy of     7: 64% (659/1028)\n",
            "Test Accuracy of     8: 98% (960/974)\n",
            "Test Accuracy of     9: 80% (813/1009)\n",
            "\n",
            "Test Accuracy (Overall): 55% (5515/10000)\n",
            "************************************************************************* \n",
            "\n",
            "Number of Faults Injected:  133000000 \n",
            "\n",
            "Test Loss: 0.92318009\n",
            "\n",
            "Test Accuracy of     0: 92% (905/980)\n",
            "Test Accuracy of     1:  4% (55/1135)\n",
            "Test Accuracy of     2: 31% (321/1032)\n",
            "Test Accuracy of     3:  1% (14/1010)\n",
            "Test Accuracy of     4: 97% (962/982)\n",
            "Test Accuracy of     5: 65% (585/892)\n",
            "Test Accuracy of     6: 88% (847/958)\n",
            "Test Accuracy of     7: 51% (526/1028)\n",
            "Test Accuracy of     8: 98% (963/974)\n",
            "Test Accuracy of     9: 58% (587/1009)\n",
            "\n",
            "Test Accuracy (Overall): 57% (5765/10000)\n",
            "************************************************************************* \n",
            "\n",
            "Number of Faults Injected:  134000000 \n",
            "\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "source": [
        "capsule_net.load_state_dict(torch.load('./model-parameters.pt'))\r\n",
        "\r\n",
        "capsule_net.eval()\r\n",
        "\r\n",
        "state_dict = capsule_net.state_dict()\r\n",
        "\r\n",
        "print(state_dict['primary_capsules.capsules.0.weight'].size())\r\n",
        "print(state_dict['primary_capsules.capsules.1.weight'].size())\r\n",
        "\r\n",
        "temp = torch.cat((state_dict['primary_capsules.capsules.0.weight'],\r\n",
        "                    state_dict['primary_capsules.capsules.1.weight'],\r\n",
        "                    state_dict['primary_capsules.capsules.2.weight'],\r\n",
        "                    state_dict['primary_capsules.capsules.3.weight'],\r\n",
        "                    state_dict['primary_capsules.capsules.4.weight'],\r\n",
        "                    state_dict['primary_capsules.capsules.5.weight'],\r\n",
        "                    state_dict['primary_capsules.capsules.6.weight'], \r\n",
        "                    state_dict['primary_capsules.capsules.7.weight']), dim=0)\r\n",
        "\r\n",
        "print(temp.size())\r\n",
        "\r\n",
        "torch.reshape(temp, (-1,)).size()\r\n",
        "\r\n",
        "final = torch.split(temp, 32)\r\n",
        "\r\n",
        "print(final[7].size())\r\n",
        "\r\n",
        "count = 0\r\n",
        "\r\n",
        "for i in range (663552):\r\n",
        "\r\n",
        "    if(torch.reshape(state_dict['primary_capsules.capsules.7.weight'], (-1,))[i] == torch.reshape(final[7], (-1,))[i]):\r\n",
        "        count += 1\r\n",
        "\r\n",
        "print(count)\r\n",
        "    #binary_primary_caps_0 = float2bit(state_dict['primary_capsules.capsules.0.weight'], num_e_bits=8, num_m_bits=23, bias=127.)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 256, 9, 9])\n",
            "torch.Size([32, 256, 9, 9])\n",
            "torch.Size([256, 256, 9, 9])\n",
            "torch.Size([32, 256, 9, 9])\n",
            "663552\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "a = np.zeros((3, 3))\r\n",
        "print (a)\r\n",
        "\r\n",
        "iterator = 47185919\r\n",
        "i = 1\r\n",
        "while i < iterator:\r\n",
        "    print(i)\r\n",
        "    i *= 2"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Firstly, we get the model stats and parameters: \r\n",
        "\r\n",
        "#from torchsummary import summary\r\n",
        "#from time import sleep\r\n",
        "\r\n",
        "capsule_net.load_state_dict(torch.load('./model-parameters.pt'))\r\n",
        "capsule_net.eval()\r\n",
        "\r\n",
        "#summary(capsule_net, input_size=(1, 28, 28))\r\n",
        "\r\n",
        "state_dict = capsule_net.state_dict()\r\n",
        "\r\n",
        "original_parameters = capsule_net.state_dict()\r\n",
        "\r\n",
        "#print('The weight tensor size is:', state_dict['digit_capsules.W'].size(), state_dict['digit_capsules.W'].dtype, '\\n')\r\n",
        "\r\n",
        "#print('Number zero weights count: ', torch.count_nonzero(weight_tensor), '\\n')\r\n",
        "\r\n",
        "#print('Number of all tensor cells: ', torch.numel(weight_tensor), '\\n')\r\n",
        "\r\n",
        "#print('Number of total bits in the weight tensor: ', f\"{torch.numel(weight_tensor)*32:,}\", 'bits', '\\n' )\r\n",
        "\r\n",
        "\r\n",
        "import random\r\n",
        "import math\r\n",
        "\r\n",
        "count = 0\r\n",
        "for i in range (1000):\r\n",
        "    #w_index = random.randint(0,9)\r\n",
        "    #x_index = random.randint(0,1151)\r\n",
        "    #y_index = random.randint(0,7)\r\n",
        "    #z_index = random.randint(0,15)\r\n",
        "    random_tensor_index = random.randint(0,1474559)\r\n",
        "    \r\n",
        "    tensor_cell = torch.reshape(state_dict['digit_capsules.W'] , (-1,))[random_tensor_index].item()\r\n",
        "\r\n",
        "    #tensor_cell = state_dict['digit_capsules.W'][w_index, x_index, y_index, z_index].item()\r\n",
        "\r\n",
        "    #print('The random selected weight at:', random_tensor_index, ' value:', type(tensor_cell), tensor_cell, '\\n')\r\n",
        "\r\n",
        "    temp = float_to_bin(tensor_cell)\r\n",
        "\r\n",
        "    #print('The random selected weight value in bits:', temp, type(temp), '\\n')\r\n",
        "\r\n",
        "    #print('Reversion check: ', bin_to_float(temp), type(bin_to_float(temp)), '\\n')\r\n",
        "\r\n",
        "    #for j in range (32):\r\n",
        "    random_bit_location = random.randint(0, 31)\r\n",
        "\r\n",
        "    #print('The selected bit is located at bit:', random_bit_location, '-  Value is:', temp[random_bit_location], '\\n')\r\n",
        "\r\n",
        "    editable_string = list(temp)\r\n",
        "        \r\n",
        "    if ( temp[random_bit_location] == '0' ):\r\n",
        "        editable_string[random_bit_location] = '1'\r\n",
        "    if ( temp[random_bit_location] == '1' ):\r\n",
        "        editable_string[random_bit_location] = '0'\r\n",
        "\r\n",
        "    final_bit_string = ''.join(editable_string)\r\n",
        "    \r\n",
        "    #print('The random selected weight value after bit flip: ', final_bit_string, '\\n')\r\n",
        "\r\n",
        "    final_tensor_value = float(bin_to_float(final_bit_string))\r\n",
        "    if (math.isfinite(final_tensor_value) == True):\r\n",
        "        print(final_tensor_value)\r\n",
        "        torch.reshape(state_dict['digit_capsules.W'] , (-1,))[random_tensor_index] = final_tensor_value\r\n",
        "    #print('The random selected weight value after bit flip in float format', final_tensor_value, '\\n')\r\n",
        "    #print(final_tensor_value)\r\n",
        "    #torch.reshape(state_dict['digit_capsules.W'] , (-1,))[random_tensor_index] = tensor_cell\r\n",
        "\r\n",
        "    #state_dict['digit_capsules.W'][w_index, x_index, y_index, z_index] = final_tensor_value\r\n",
        "    #sleep(0.5) # Time in seconds\r\n",
        "    #Loading updated faulty cell back into the model's tensor\r\n",
        "\r\n",
        "\r\n",
        "        \r\n",
        "\r\n",
        "capsule_net.load_state_dict(state_dict)\r\n",
        "\r\n",
        "capsule_net.eval()\r\n",
        "caps_output, images, reconstructions = test(capsule_net, test_loader)\r\n",
        "\r\n",
        "  #capsule_net.load_state_dict(original_parameters)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(torch.reshape(state_dict['digit_capsules.W'] , (-1,))[0].item())\r\n",
        "torch.reshape(state_dict['digit_capsules.W'] , (-1,))[0] = 0\r\n",
        "print(torch.reshape(state_dict['digit_capsules.W'] , (-1,))[0].item())\r\n",
        "\r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import math\r\n",
        "max = 0\r\n",
        "min = 0 \r\n",
        "for i in range (1474559):\r\n",
        "    if (torch.reshape(state_dict['digit_capsules.W'] , (-1,))[i].item() > max):\r\n",
        "        max = torch.reshape(state_dict['digit_capsules.W'] , (-1,))[i].item()\r\n",
        "\r\n",
        "    if (torch.reshape(state_dict['digit_capsules.W'] , (-1,))[i].item() < min):\r\n",
        "        min = torch.reshape(state_dict['digit_capsules.W'] , (-1,))[i].item()\r\n",
        "\r\n",
        "print(min)\r\n",
        "print(max)\r\n",
        "\r\n",
        "count = 0\r\n",
        "\r\n",
        "for i in range (1474559):\r\n",
        "    if (math.isfinite(torch.reshape(state_dict['digit_capsules.W'] , (-1,))[i].item()) == True):\r\n",
        "        count = count + 1\r\n",
        "\r\n",
        "print(count)\r\n",
        "#caps_output, images, reconstructions = test(capsule_net, test_loader)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import math\r\n",
        "if (bin_to_float('01111111100000000000000000000000') == float('inf')):\r\n",
        "    print('yes')\r\n",
        "\r\n",
        "print(math.isfinite(bin_to_float('01111111100000000000000000000000')))\r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(caps_output.size())"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def display_images(images, reconstructions):\r\n",
        "    '''Plot one row of original MNIST images and another row (below) \r\n",
        "       of their reconstructions.'''\r\n",
        "    # convert to numpy images\r\n",
        "    images = images.data.cpu().numpy()\r\n",
        "    reconstructions = reconstructions.view(-1, 1, 28, 28)\r\n",
        "    reconstructions = reconstructions.data.cpu().numpy()\r\n",
        "    \r\n",
        "    # plot the first ten input images and then reconstructed images\r\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(26,5))\r\n",
        "\r\n",
        "    # input images on top row, reconstructions on bottom\r\n",
        "    for images, row in zip([images, reconstructions], axes):\r\n",
        "        for img, ax in zip(images, row):\r\n",
        "            ax.imshow(np.squeeze(img), cmap='gray')\r\n",
        "            ax.get_xaxis().set_visible(False)\r\n",
        "            ax.get_yaxis().set_visible(False)"
      ],
      "outputs": [],
      "metadata": {
        "id": "HnjrQyqS1jem"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# display original and reconstructed images, in rows\r\n",
        "display_images(images, reconstructions)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "ZTa-sNhX1nf9",
        "outputId": "e168f284-4bb3-441e-b414-6762b81fae86"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# convert data to Tensor *and* perform random affine transformation\r\n",
        "transform = transforms.Compose(\r\n",
        "    [transforms.RandomAffine(degrees=30, translate=(0.1,0.1)),\r\n",
        "     transforms.ToTensor()]\r\n",
        "    )\r\n",
        "\r\n",
        "# test dataset\r\n",
        "transformed_test_data = datasets.MNIST(root='data', train=False,\r\n",
        "                                       download=True, transform=transform)\r\n",
        "\r\n",
        "# prepare data loader\r\n",
        "transformed_test_loader = torch.utils.data.DataLoader(transformed_test_data, \r\n",
        "                                                      batch_size=batch_size,\r\n",
        "                                                      num_workers=num_workers)"
      ],
      "outputs": [],
      "metadata": {
        "id": "9vFtJK0R1sHv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# obtain one batch of test images\r\n",
        "dataiter = iter(transformed_test_loader)\r\n",
        "images, labels = dataiter.next()\r\n",
        "images = images.numpy()\r\n",
        "\r\n",
        "# plot the images in the batch, along with the corresponding labels\r\n",
        "fig = plt.figure(figsize=(25, 4))\r\n",
        "for idx in np.arange(batch_size):\r\n",
        "    ax = fig.add_subplot(2, batch_size/2, idx+1, xticks=[], yticks=[])\r\n",
        "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\r\n",
        "    # print out the correct label for each image\r\n",
        "    # .item() gets the value contained in a Tensor\r\n",
        "    ax.set_title(str(labels[idx].item()))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "KckPo33p1v2S",
        "outputId": "ddf44b2d-93b2-4d56-ad94-ae6dd680b1ba"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\r\n",
        "# call test function and get reconstructed images\r\n",
        "_, images, reconstructions = test(capsule_net, transformed_test_loader)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd3PdXRe12zL",
        "outputId": "9a600558-8041-4a45-ec1a-960d91fdf2e2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# original input images\r\n",
        "display_images(images, reconstructions)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "XIlrGK7C1-GP",
        "outputId": "d57502ec-1ea2-42c4-8897-8837c94f2044"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from IPython.display import FileLink\r\n",
        "FileLink(r'Q-mnist.pt')"
      ],
      "outputs": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "a344be94937486b3a7f1f0c255bb269d6d9cd5bd89a7c219d69b8756ffbd242d"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.11 64-bit ('pytorch': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}